# VPKE_mian
Title: "VisualRAG: Knowledge-Guided Retrieval Augmentation for Image-Text Matching"

Current research continues to advance with ongoing in-depth exploration, and the full release of complete code may experience some delays. We appreciate your understanding. For now, we are prioritizing the disclosure of key preprocessed datasets from the study. These resources are made available to facilitate researchers in related fields and encourage the emergence of innovative research approaches.

* The required dataset files can be found [here](https://drive.google.com/drive/folders/1TKucwpCcKdPlby6JpAKgjxT8V0uIqwrK?usp=sharing). You can download the original image datasets from the official websites of [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) and [MS-COCO](https://cocodataset.org/#home).

* To improve training efficiency, we pre-generate auxiliary text pools using [BLIP-2](https://huggingface.co/Salesforce/collections) (ready for direct loading), which can also be generated by running ``` data_gen.py ``` if needed. The external knowledge base keyword files (currently being organized) support dynamic construction via ``` sele_key.py ```, and preprocessing is recommended. This preprocessing pipeline employs a staged design, allowing flexible adjustment of computational scale based on available GPU memory capacity.

This code repository is partially built upon the excellent works of [X-Pool](https://github.com/layer6ai-labs/xpool), [CLIP4Clip](https://github.com/ArrowLuo/CLIP4Clip/tree/master), and [ESL](https://github.com/kkzhang95/ESL/tree/main). We sincerely appreciate these contributors for their valuable work.  
